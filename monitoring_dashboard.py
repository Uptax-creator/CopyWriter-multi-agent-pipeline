#!/usr/bin/env python3
"""
ğŸ” Dashboard de Monitoramento AvanÃ§ado - Ciclo C Fase 1
Monitoramento em tempo real do sistema Omie FastMCP
"""

import asyncio
import json
import psutil
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List
from fastmcp import FastMCP
import httpx
from pathlib import Path

# Servidor de Monitoramento
monitoring = FastMCP("Omie Monitoring Dashboard ğŸ“Š")

# ConfiguraÃ§Ãµes
CONFIG = {
    "alert_thresholds": {
        "cpu_percent": 80,
        "memory_percent": 85,
        "disk_percent": 90,
        "response_time_ms": 5000
    },
    "check_interval": 30,  # segundos
    "history_retention": 24  # horas
}

# Armazenamento em memÃ³ria para mÃ©tricas histÃ³ricas
metrics_history: List[Dict[str, Any]] = []

@monitoring.tool
async def get_system_metrics() -> str:
    """
    Coleta mÃ©tricas completas do sistema em tempo real
    
    Returns:
        str: MÃ©tricas do sistema em formato JSON
    """
    try:
        # MÃ©tricas bÃ¡sicas do sistema
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network = psutil.net_io_counters()
        
        # MÃ©tricas de processo
        process = psutil.Process()
        process_memory = process.memory_info()
        
        # ConexÃµes ativas
        connections = len(psutil.net_connections(kind='inet'))
        
        # Timestamp
        timestamp = datetime.now().isoformat()
        
        metrics = {
            "timestamp": timestamp,
            "system": {
                "cpu_percent": cpu_percent,
                "memory": {
                    "total_gb": round(memory.total / (1024**3), 2),
                    "available_gb": round(memory.available / (1024**3), 2),
                    "percent_used": memory.percent,
                    "free_gb": round(memory.free / (1024**3), 2)
                },
                "disk": {
                    "total_gb": round(disk.total / (1024**3), 2),
                    "used_gb": round(disk.used / (1024**3), 2),
                    "free_gb": round(disk.free / (1024**3), 2),
                    "percent_used": round((disk.used / disk.total) * 100, 2)
                },
                "network": {
                    "bytes_sent": network.bytes_sent,
                    "bytes_recv": network.bytes_recv,
                    "packets_sent": network.packets_sent,
                    "packets_recv": network.packets_recv
                },
                "connections": connections
            },
            "process": {
                "memory_rss_mb": round(process_memory.rss / (1024**2), 2),
                "memory_vms_mb": round(process_memory.vms / (1024**2), 2),
                "cpu_percent": process.cpu_percent(),
                "num_threads": process.num_threads()
            },
            "alerts": await check_alerts(cpu_percent, memory.percent, disk.used/disk.total*100)
        }
        
        # Adicionar ao histÃ³rico
        global metrics_history
        metrics_history.append(metrics)
        
        # Manter apenas as Ãºltimas 24 horas
        cutoff_time = datetime.now() - timedelta(hours=CONFIG["history_retention"])
        metrics_history = [
            m for m in metrics_history 
            if datetime.fromisoformat(m["timestamp"]) > cutoff_time
        ]
        
        return json.dumps(metrics, indent=2, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "error": f"Erro ao coletar mÃ©tricas: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }, indent=2)

@monitoring.tool
async def health_check_complete() -> str:
    """
    Health check completo do sistema Omie FastMCP
    
    Returns:
        str: Status detalhado de saÃºde do sistema
    """
    try:
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "unknown",
            "checks": {}
        }
        
        # 1. Teste de conectividade Omie API
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                response = await client.get("https://app.omie.com.br/api/v1/")
                health_status["checks"]["omie_api"] = {
                    "status": "ok" if response.status_code < 400 else "error",
                    "response_time_ms": response.elapsed.total_seconds() * 1000,
                    "status_code": response.status_code
                }
        except Exception as e:
            health_status["checks"]["omie_api"] = {
                "status": "error",
                "error": str(e)
            }
        
        # 2. VerificaÃ§Ã£o de recursos do sistema
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent
        disk_percent = psutil.disk_usage('/').percent
        
        health_status["checks"]["system_resources"] = {
            "cpu_ok": cpu_percent < CONFIG["alert_thresholds"]["cpu_percent"],
            "memory_ok": memory_percent < CONFIG["alert_thresholds"]["memory_percent"],
            "disk_ok": disk_percent < CONFIG["alert_thresholds"]["disk_percent"],
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "disk_percent": disk_percent
        }
        
        # 3. VerificaÃ§Ã£o de arquivos crÃ­ticos
        critical_files = [
            "credentials.json",
            "omie_fastmcp_example.py",
            "claude_desktop_config_final.json"
        ]
        
        health_status["checks"]["critical_files"] = {}
        for file in critical_files:
            file_path = Path(file)
            health_status["checks"]["critical_files"][file] = {
                "exists": file_path.exists(),
                "size_bytes": file_path.stat().st_size if file_path.exists() else 0
            }
        
        # 4. VerificaÃ§Ã£o de processos MCP
        mcp_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['cmdline'] and any('mcp' in str(arg).lower() for arg in proc.info['cmdline']):
                    mcp_processes.append({
                        "pid": proc.info['pid'],
                        "name": proc.info['name'],
                        "cmdline": " ".join(proc.info['cmdline'][:3])  # Primeiros 3 args
                    })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        health_status["checks"]["mcp_processes"] = {
            "count": len(mcp_processes),
            "processes": mcp_processes
        }
        
        # Determinar status geral
        all_checks_ok = (
            health_status["checks"]["omie_api"].get("status") == "ok" and
            health_status["checks"]["system_resources"]["cpu_ok"] and
            health_status["checks"]["system_resources"]["memory_ok"] and
            health_status["checks"]["system_resources"]["disk_ok"] and
            len(mcp_processes) > 0
        )
        
        health_status["overall_status"] = "healthy" if all_checks_ok else "warning"
        
        return json.dumps(health_status, indent=2, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "error": f"Erro no health check: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "overall_status": "error"
        }, indent=2)

@monitoring.tool
async def get_metrics_history(hours: int = 1) -> str:
    """
    Retorna histÃ³rico de mÃ©tricas para anÃ¡lise de tendÃªncias
    
    Args:
        hours: NÃºmero de horas de histÃ³rico a retornar
        
    Returns:
        str: HistÃ³rico de mÃ©tricas em formato JSON
    """
    try:
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        filtered_history = [
            m for m in metrics_history 
            if datetime.fromisoformat(m["timestamp"]) > cutoff_time
        ]
        
        # EstatÃ­sticas resumidas
        if filtered_history:
            cpu_values = [m["system"]["cpu_percent"] for m in filtered_history]
            memory_values = [m["system"]["memory"]["percent_used"] for m in filtered_history]
            
            summary = {
                "period_hours": hours,
                "data_points": len(filtered_history),
                "summary_stats": {
                    "cpu": {
                        "avg": round(sum(cpu_values) / len(cpu_values), 2),
                        "max": max(cpu_values),
                        "min": min(cpu_values)
                    },
                    "memory": {
                        "avg": round(sum(memory_values) / len(memory_values), 2),
                        "max": max(memory_values),
                        "min": min(memory_values)
                    }
                },
                "data": filtered_history[-50:]  # Ãšltimos 50 pontos para nÃ£o sobrecarregar
            }
        else:
            summary = {
                "period_hours": hours,
                "data_points": 0,
                "message": "Nenhum dado disponÃ­vel para o perÃ­odo solicitado",
                "data": []
            }
        
        return json.dumps(summary, indent=2, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "error": f"Erro ao buscar histÃ³rico: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }, indent=2)

async def check_alerts(cpu: float, memory: float, disk: float) -> List[Dict[str, Any]]:
    """
    Verifica condiÃ§Ãµes de alerta baseadas nos thresholds configurados
    
    Args:
        cpu: Percentual de uso da CPU
        memory: Percentual de uso da memÃ³ria
        disk: Percentual de uso do disco
        
    Returns:
        List[Dict]: Lista de alertas ativos
    """
    alerts = []
    thresholds = CONFIG["alert_thresholds"]
    
    if cpu > thresholds["cpu_percent"]:
        alerts.append({
            "type": "cpu_high",
            "severity": "warning",
            "message": f"CPU usage high: {cpu}% (threshold: {thresholds['cpu_percent']}%)",
            "value": cpu,
            "threshold": thresholds["cpu_percent"]
        })
    
    if memory > thresholds["memory_percent"]:
        alerts.append({
            "type": "memory_high",
            "severity": "warning",
            "message": f"Memory usage high: {memory}% (threshold: {thresholds['memory_percent']}%)",
            "value": memory,
            "threshold": thresholds["memory_percent"]
        })
    
    if disk > thresholds["disk_percent"]:
        alerts.append({
            "type": "disk_high",
            "severity": "critical",
            "message": f"Disk usage high: {disk}% (threshold: {thresholds['disk_percent']}%)",
            "value": disk,
            "threshold": thresholds["disk_percent"]
        })
    
    return alerts

@monitoring.tool
async def configure_alerts(
    cpu_threshold: int = 80,
    memory_threshold: int = 85,
    disk_threshold: int = 90,
    response_time_threshold: int = 5000
) -> str:
    """
    Configura thresholds para os alertas do sistema
    
    Args:
        cpu_threshold: Threshold CPU em %
        memory_threshold: Threshold memÃ³ria em %
        disk_threshold: Threshold disco em %
        response_time_threshold: Threshold tempo resposta em ms
        
    Returns:
        str: ConfirmaÃ§Ã£o da configuraÃ§Ã£o
    """
    global CONFIG
    
    CONFIG["alert_thresholds"] = {
        "cpu_percent": cpu_threshold,
        "memory_percent": memory_threshold,
        "disk_percent": disk_threshold,
        "response_time_ms": response_time_threshold
    }
    
    return json.dumps({
        "message": "Alert thresholds updated successfully",
        "new_config": CONFIG["alert_thresholds"],
        "timestamp": datetime.now().isoformat()
    }, indent=2)

# Recursos para dados estruturados
@monitoring.resource("monitoring://dashboard")
async def dashboard_data() -> str:
    """
    Dados estruturados para dashboard de monitoramento
    
    Returns:
        str: Dados do dashboard em formato JSON
    """
    try:
        # MÃ©tricas atuais
        current_metrics = await get_system_metrics()
        metrics_data = json.loads(current_metrics)
        
        # Health check
        health_data = json.loads(await health_check_complete())
        
        # Ãšltimas 2 horas de histÃ³rico
        history_data = json.loads(await get_metrics_history(2))
        
        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "status": health_data.get("overall_status", "unknown"),
            "current_metrics": metrics_data,
            "health_check": health_data,
            "trends": history_data,
            "alerts_active": len(metrics_data.get("alerts", [])),
            "uptime_hours": round(time.time() - psutil.boot_time()) / 3600
        }
        
        return json.dumps(dashboard, indent=2, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "error": f"Erro ao gerar dados do dashboard: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }, indent=2)

@monitoring.resource("monitoring://alerts")
async def active_alerts() -> str:
    """
    Lista de alertas ativos no sistema
    
    Returns:
        str: Alertas ativos em formato JSON
    """
    try:
        # Coletar mÃ©tricas atuais para verificar alertas
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent
        disk_percent = psutil.disk_usage('/').percent
        
        alerts = await check_alerts(cpu_percent, memory_percent, disk_percent)
        
        return json.dumps({
            "timestamp": datetime.now().isoformat(),
            "alert_count": len(alerts),
            "alerts": alerts,
            "status": "critical" if any(a["severity"] == "critical" for a in alerts) else "warning" if alerts else "ok"
        }, indent=2)
        
    except Exception as e:
        return json.dumps({
            "error": f"Erro ao buscar alertas: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }, indent=2)

# Prompt para anÃ¡lise de performance
@monitoring.prompt("performance-analysis")
async def performance_analysis_prompt(period_hours: str = "1") -> str:
    """
    Template para anÃ¡lise de performance do sistema
    
    Args:
        period_hours: PerÃ­odo em horas para anÃ¡lise
        
    Returns:
        str: Prompt formatado para anÃ¡lise
    """
    return f"""
Analise o desempenho do sistema Omie FastMCP nas Ãºltimas {period_hours} horas.

ğŸ“Š DADOS PARA ANÃLISE:
1. Use get_metrics_history({period_hours}) para obter dados histÃ³ricos
2. Use health_check_complete() para status atual
3. Use monitoring://dashboard para visÃ£o geral
4. Use monitoring://alerts para alertas ativos

ğŸ” ANÃLISE REQUERIDA:
1. Identifique tendÃªncias de CPU, memÃ³ria e disco
2. Detecte picos de uso anÃ´malos
3. Avalie performance da API Omie
4. Verifique alertas ativos e sua frequÃªncia
5. Sugira otimizaÃ§Ãµes especÃ­ficas

ğŸ“‹ RELATÃ“RIO:
- Resumo executivo da performance
- Principais mÃ©tricas e tendÃªncias
- Problemas identificados e impacto
- RecomendaÃ§Ãµes de aÃ§Ã£o imediata
- SugestÃµes de melhoria de longo prazo
"""

if __name__ == "__main__":
    print("ğŸ” Dashboard de Monitoramento Omie FastMCP")
    print("ğŸ“Š Ferramentas disponÃ­veis:")
    print("   - get_system_metrics: MÃ©tricas em tempo real")
    print("   - health_check_complete: Health check completo")
    print("   - get_metrics_history: HistÃ³rico de mÃ©tricas")
    print("   - configure_alerts: Configurar alertas")
    print("ğŸ“‚ Recursos disponÃ­veis:")
    print("   - monitoring://dashboard: Dados dashboard")
    print("   - monitoring://alerts: Alertas ativos")
    print("ğŸ“ Prompts disponÃ­veis:")
    print("   - performance-analysis: AnÃ¡lise de performance")
    print()
    
    # Executar servidor de monitoramento
    monitoring.run()